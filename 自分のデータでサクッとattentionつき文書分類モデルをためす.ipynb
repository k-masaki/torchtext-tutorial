{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "Deep Learning系の技術が流行っていますが、画像認識などの技術に比べて、機械翻訳や文書分類などの自然言語処理系の技術はとっつきにくいと考えられているようです。その原因の大部分を前処理のめんどくささが占めています。どのDeep LearningフレームワークにもLSTMなどのtextを扱うモデルのチュートリアルがあるのですが、現状\n",
    "\n",
    "+ 簡単すぎるtoyモデルを題材としている\n",
    "+ build in の前処理済みベンチマーク用データセットを題材としている\n",
    "+ neural networkのコーディングに主眼を置おり、データ整備に関する記述は少ない\n",
    "+ ほとんどが英語のデータを扱っており、日本語データを扱うチュートリアルは少ない\n",
    "\n",
    "という問題点があり、公式のチュートリアルだけでは**「自分で用意した日本語データを使用して、Deep Learningモデルをパッと試してみる」**ことができるようになるまでギャップがある気がします。\n",
    "\n",
    "そこで、この記事ではTorchtextというpythonライブラリを中心に、次のような内容を目指します。\n",
    "\n",
    "+ さまざまな自然言語処理系のDeep Learningモデル作成に共通して必要なデータ処理フローを整理する。\n",
    "+ torchtextを利用してそれらのデータ処理が少ないコーディング量で可能になることを説明する。\n",
    "+ 英語だけではなく日本語のtextデータも扱える方法を解説する。\n",
    "+ 読者が**自分で用意したtextデータでDLモデルをさくっと試す**ことが出来るようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchtext とは\n",
    "torchtext とは自然言語処理関連の前処理を簡単にやってくれる非常に優秀なライブラリです。自分も業務で自然言語処理がからむDeep Learningモデルを構築するときなど大変お世話になっています。torchとついていますが、Pytorchからだけではなく、Tensorflowなどの他のライブラリと一緒に使うこともできます。\n",
    "\n",
    "torchtextは強力なライブラリなのですが、英語も含めてdocumentは充実していません。既存の記事だと\n",
    "\n",
    "+ [A Tutorial on Torchtext](http://anie.me/On-Torchtext/)\n",
    "+ [TorchTextTutorial](https://github.com/mjc92/TorchTextTutorial/blob/master/01.%20Getting%20started.ipynb)\n",
    "+ [torchtextのソースコードを読んでみた](http://hacks.deeplearning.jp/torchtext/)\n",
    "\n",
    "が参考になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textデータをDeep Learningモデルに入力する際の典型的なデータフロー\n",
    "自然言語処理のタスクは、文書分類や機械翻訳などたくさんあります。Deep Learningのフレームワークもpytorchやtensorflowなどたくさんあります。しかし、どのタスクをどのフレームワークであつかう場合もtextデータをDeep Neural Networkに入力する際は、たいてい\n",
    "1. センテンスを単語ごとに区切り、\n",
    "2. 単語に番号を振って、\n",
    "3. センテンスを表す番号の列をミニバッチごとにまとめた行列の形\n",
    "にして渡す必要があります。\n",
    "\n",
    "![flow.png](https://qiita-image-store.s3.amazonaws.com/0/183955/ff027e0f-7cd1-6a76-0eaa-05b8ae61074e.png)\n",
    "\n",
    "\n",
    "この図の0:テキストデータ読み込み～4:単語埋め込みまでのデータフローの管理をしてくれるのがtorchtextです。\n",
    "単語埋め込みの部分はDeep learningフレームワークの方で扱う場合が多いですが、学習済み埋め込みベクトルの管理はtorchtextで行えます。\n",
    "\n",
    "つまり、テキストデータを読み込んで、ミニバッチごとに上図の4の形式の行列を出力するようなiteratorを作成するのがtorchtextの目標です。\n",
    "\n",
    "以降の章で、上の図の各ステップをtorchtextでどのように記述するのか、説明していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用データ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_ja.tsv\n",
    "```\n",
    "あなたを愛しています。\t1\n",
    "私はマイクが嫌いです。\t0\n",
    "私はマキがすきです。\t1\n",
    "私はあなたが好きではありません。\t0\n",
    "```\n",
    "1カラム目がテキスト、2カラム目がラベル(0スタート)のtsvファイルを用意します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手順0, 1:データの読み込みと分かち書き\n",
    "まず初めにデータを読み込んで、単語に分割(tokenize)する処理に関して説明します。データのtokenizationにかかわってくるのがFieldクラスです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fieldクラス\n",
    "Fieldクラスは読み込んだデータに施す前処理とその結果を管理するクラスです。\n",
    "torchtextでは、データを取り込む際、\n",
    "\n",
    "1. 各データソースの各カラムに対して前処理を管理するFieldクラスを指定\n",
    "2. 各カラムごとに、指定されたFieldクラスが管理する前処理が実行される\n",
    "\n",
    "という流れになります。Fieldクラスが管理する前処理にはテキストのtokenizationも含まれます。\n",
    "\n",
    "文書分類モデルだと、次のようにtext用のTEXTフィールドとラベル用のLABELフィールドを用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_t = Tokenizer()\n",
    "def tokenizer(text): \n",
    "    return [tok for tok in j_t.tokenize(text, wakati=True)]\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.Fieldの各引数は次のような意味を持ちます。\n",
    "\n",
    "+ sequential: 対応するデータがtextのように可変長のデータかどうか\n",
    "+ lower: 文字をすべて小文字に変換するかどうか\n",
    "+ tokenize: tokenizeや前処理に使用する関数\n",
    "+ include_length: テキストの長さデータを保持するか\n",
    "+ データ数方向の次元を1番目にもってくるか\n",
    "\n",
    "tokenizeに使用する関数は文字列を受け取り、tokenize結果の配列を返す関数です。  \n",
    "ここに記述した処理が、データを読み込んだ時に各カラムの各行に対して適用されます。  \n",
    "次のよな挙動を想定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['あなた', 'を', '愛し', 'て', 'い', 'ます', '。']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(u'あなたを愛しています。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textデータのクリーニングに関して\n",
    "textデータのクリーニングに関しては次の記事が参考になります。\n",
    "[自然言語処理における前処理の種類とその威力](https://qiita.com/Hironsan/items/2466fe0f344115aff177)\n",
    "この記事で記述されているクリーニング系の処理のうち、\n",
    "\n",
    "+ 英語の大文字小文字変換\n",
    "+ 出現頻度の低い単語の除去\n",
    "\n",
    "はtorchtextの機能として実装されています。英語の大文字小文字変換は上のようにFieldのlower引数によって制御します。出現頻度の低い単語の除去は後の章で説明します。\n",
    "その他のクリーニング処理はあらかじめ入力tsvファイル作成時に行ってもよいですが、htmlタグの除去などは次のようにFieldオブジェクトのtokenizer関数内に記述し、データの読み込みと同時に行うこともできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['あなた', 'を', '愛し', 'て', 'い', 'ます', '。']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_tokenizer(text):\n",
    "    soup = BeautifulSoup(text,\"lxml\")\n",
    "    clean_text = soup.get_text()\n",
    "    return [tok for tok in j_t.tokenize(clean_text, wakati=True)]\n",
    "\n",
    "print(clean_tokenizer(u'<p>あなたを愛しています。</p>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み\n",
    "データパスと各カラムに対応するFieldを指定してデータを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='./', train='train_ja.tsv',\n",
    "        validation='val_ja.tsv', test='test_ja.tsv', format='tsv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.TabularDataset.splitsはデータを読み込んで、datasetオブジェクトを返します。  \n",
    "このような形でfield引数にFieldオブジェクトを渡すと、train_ja.tsvの1列目にTEXT Fieldの、2列目にLABEL Fieldのtokenizerに記述された前処理＋単語分割処理が適用されます。そして、それぞれ返り値のdatasetオブジェクトに'Text','Label'という名前で格納されます。\n",
    "結果は次のように確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 4\n",
      "vars(train[0]) {'Text': ['あなた', 'を', 'が', '好き', 'です', '。'], 'Label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手順2:単語へ番号を振る\n",
    "読み込んだデータに出現した単語のリストを作成し、単語に番号を振ります。Fieldクラスのbuild_vocabメソッドを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のコードでは次のような処理が行われます。\n",
    "\n",
    "1. データ内の各単語の数をカウントし、TEXT.vocab.freqsに格納\n",
    "2. min_freq以上の回数出現した単語に番号を振り、番号から単語への辞書をTEXT.vocab.itosに、単語から番号への辞書をTEXT.vocab.stoiに格納\n",
    "\n",
    "単語カウントや番号を振った結果は次のように確認できます。\n",
    "\n",
    "単語カウント結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'。': 4,\n",
       "         'あなた': 1,\n",
       "         'が': 4,\n",
       "         'です': 4,\n",
       "         'は': 2,\n",
       "         'を': 1,\n",
       "         'ボブ': 1,\n",
       "         'マイク': 1,\n",
       "         'マキ': 1,\n",
       "         '好き': 2,\n",
       "         '嫌い': 2,\n",
       "         '私': 2})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語-番号辞書"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index>,\n",
       "            {'<pad>': 1,\n",
       "             '<unk>': 0,\n",
       "             '。': 2,\n",
       "             'が': 3,\n",
       "             'です': 4,\n",
       "             'は': 5,\n",
       "             '好き': 6,\n",
       "             '嫌い': 7,\n",
       "             '私': 8})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_freqを2としたので、出現回数が2未満の単語は未知語unk扱いになります。\n",
    "padはバッチ作成時に使用するダミートークンで、後の章で説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "番号-単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '。', 'が', 'です', 'は', '好き', '嫌い', '私']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済み単語ベクトルについて\n",
    "build_vocabメソッドのvectors引数を使用すると、単語へ番号を振るのと同時に、学習済みの単語ベクトルを指定し、読み込むことができます。\n",
    "単語ベクトルは、その言語のすべての単語について保持しておく必要はなく、学習データにおける出現回数がmin_freq以上の単語に対応するベクトルのみメモリ上に保持します。そのため、torchtextでは上記のように単語の出現回数のカウントを行うbuild_vocabメソッドで学習済みの単語ベクトルを読み込んでいます。\n",
    "独自のベクトル群を指定することもできれば、webで公開されている学習済みベクトルをダウンロードしてくることも可能です。公開されている単語ベクトルの内torchtextが対応しているものは[glove](https://nlp.stanford.edu/projects/glove/)と[fasttext](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)と[charngram](http://www.logos.t.u-tokyo.ac.jp/~hassy/publications/arxiv2016jmt/)です。このうち、日本語にも対応しているのはfasttextです。fasttextのベクトルをダウンロードして来て、読み込むには"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=FastText(language=\"ja\"), min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とします。\n",
    "このように指定すると、単語に番号を振る処理に加えて\n",
    "\n",
    "1. .vector_cache/以下をチェック\n",
    "2. ダウンロード済みでなければ.vector_cache/以下にfasttextで学習したベクトル群をダウンロード\n",
    "3. trainにおける出現回数がmin_freq以上の単語に関して、学習済み単語ベクトルを読み込みTEXT.vocab.vectorsに格納\n",
    "\n",
    "という処理が走ります。\n",
    "TEXT.vocab.vectorsは\n",
    "\n",
    "+ 行数:min_freq以上出現した単語数+特殊トークン数(\\<unk\\> , \\<pad\\>)\n",
    "+ 列数:単語ベクトルの次元。デフォルトだとfasttextは300次元\n",
    "\n",
    "という行列になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 300])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みの単語ベクトルを使用しない場合はvectorsを指定しません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手順3:バッチ化\n",
    "\n",
    "data.Iterator.splits はdatasetオブジェクトから、各単語を番号に変換してミニバッチごとにまとめた行列を返すイテレータを作成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "        (train, val, test), batch_sizes=(2, 2, 1), device=-1, repeat=False,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "イテレータが返す結果は次のように確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "    0     0     3     6     4     2     1\n",
      "    8     5     0     3     7     4     2\n",
      "[torch.LongTensor of size 2x7]\n",
      ", \n",
      " 6\n",
      " 7\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "Variable containing:\n",
      " 1\n",
      " 0\n",
      "[torch.LongTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch.Textは単語番号の行列と、テキストデータの長さを表す配列のタプルになっています。(data.Fieldの引数でinclude_length=Trueとしたためです)RNNなどの学習で[masked BPTT](https://en.wikipedia.org/wiki/Backpropagation_through_time)を使用する際は、テキストデータのミニバッチと同時に、このテキストの長さのデータを渡します。\n",
    "学習ループでは、このbatch.Textを入力として、Deep Learningモデルによる予測を行い、予測とbatch.Labelを比べて誤差を計算して、学習アルゴリズムを回すことになります。\n",
    "\n",
    "手順2で作成した番号-単語辞書でbatch.Textを変換すると  \n",
    "\n",
    "+ 私 は あなた が unk unk は unk unk unk 。\n",
    "+ 私 は unk が unk です 。 pad pad pad pad\n",
    "\n",
    "となります。padトークンはこのようにバッチ化の際、短いtextに追加して、長さをそろえるために使用されます。デフォルトではミニバッチ内で最も長いtextの長さになるまでpaddingされます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手順4:単語埋め込みベクトルの使用\n",
    "\n",
    "手順2で読み込んだ単語埋め込みベクトルは、手順3で作成したイテレータが返す行列内の単語番号をベクトルに変換する際に使用されます。このように単語をベクトル化した結果がLSTMモジュールなどへの直接の入力となります。\n",
    "単語番号をベクトルに変換するモジュールは各Deep Learningフレームワークに用意されているはずです。(pytorchならnn.Embedding、kerasならlayers.embeddings.Embeddingを使用します。)\n",
    "例えばpytorchの場合はnn.Embeddingモジュールに次のように埋め込みベクトル(TEXT.vocab.vectors)を渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, v_size, gpu=True, batch_first=True):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.h_dim = h_dim\n",
    "        self.embed = nn.Embedding(v_size, emb_dim)\n",
    "        self.embed.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        self.lstm = nn.LSTM(emb_dim, h_dim, batch_first=batch_first,\n",
    "                            bidirectional=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手順5:attentionつき判別モデルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの準備が整ったので、文書分類モデルを構築していきます。attentionという機構を組み込んで、予測理由の可視化も行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attentionの復習\n",
    "attentionとは（正確な定義ではないですが）予測モデルに入力データのどの部分に注目するか知らせる機構のことです。\n",
    "attention技術は機械翻訳への応用が特に有名です。\n",
    "例えば、日英翻訳モデルを考えます。翻訳モデルは”これはペンです”という文字列を入力として\"This is a pen\"という英文を出力しますが、「pen」という文字を出力する際、モデルは入力文の「ペン」という文字に注目するはずです。このように入力データのある部分に「注目する=attention」という機構を予測モデルに組み込むことで、種々のタスクにおいいて精度が向上することが報告されてきました。\n",
    "また、このattentionを可視化することで「入力データのどの部分に注目して予測を行ったか」という形で予測理由の提示を行うことができます。\n",
    "attentionについての説明と実装は\n",
    "\n",
    "+ [pytorch チュートリアル](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb)\n",
    "\n",
    "がとても参考になります。\n",
    "\n",
    "# self attention を利用した分類\n",
    "今回は、attentionの技術を利用して、予測理由が可視化できる文書分類モデルを実装していきます。\n",
    "[self-attentive sentence embedding](https://arxiv.org/pdf/1703.03130.pdf) という論文の手法を単純化したものになります。\n",
    "この手法は次のような手順で予測を行います。\n",
    "\n",
    "1. bidirectional LSTMで文書を変換\n",
    "2. 各単語に対応する隠れ層(下図$h_i$)を入力とし、予測の際その単語に注目すべき確率（self attention 下図$A_i$）をNeural Networkで予測\n",
    "3. self attention の重み付で各単語に対応する隠れ層を足し合わせたものを入力とし、Neural Networkで文書のラベルを予測\n",
    "\n",
    "この$A_i$を可視化してやれば、モデルが予測の際どの単語に注目したかを知ることができます。\n",
    "(オリジナル論文では複数個のself attentionを利用する方法が提案されているのですが、今回は簡易のためattentionは1種類としています。)\n",
    "\n",
    "![image.png](https://qiita-image-store.s3.amazonaws.com/0/183955/8e7d33eb-6182-81c9-7eca-cc138e5f1e02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bidirectional lstmの部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, v_size, gpu=True, v_vec=None, batch_first=True):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.h_dim = h_dim\n",
    "        self.embed = nn.Embedding(v_size, emb_dim)\n",
    "        if v_vec is not None:\n",
    "            self.embed.weight.data.copy_(v_vec)\n",
    "        self.lstm = nn.LSTM(emb_dim, h_dim, batch_first=batch_first,\n",
    "                            bidirectional=True)\n",
    "\n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = Variable(torch.zeros(1*2, b_size, self.h_dim))\n",
    "        c0 = Variable(torch.zeros(1*2, b_size, self.h_dim))\n",
    "        if self.gpu:\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, sentence, lengths=None):\n",
    "        self.hidden = self.init_hidden(sentence.size(0))\n",
    "        emb = self.embed(sentence)\n",
    "        packed_emb = emb\n",
    "\n",
    "        if lengths is not None:\n",
    "            lengths = lengths.view(-1).tolist()\n",
    "            packed_emb = nn.utils.rnn.pack_padded_sequence(emb, lengths)\n",
    "\n",
    "        out, hidden = self.lstm(packed_emb, self.hidden)\n",
    "\n",
    "        if lengths is not None:\n",
    "            out = nn.utils.rnn.pad_packed_sequence(output)[0]\n",
    "\n",
    "        out = out[:, :, :self.h_dim] + out[:, :, self.h_dim:]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attentionクラスです。\n",
    "LSTMの隠れ層を入力として、各単語へのattentionを出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super(Attn, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(h_dim, 24),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(24,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        b_size = encoder_outputs.size(0)\n",
    "        attn_ene = self.main(encoder_outputs.view(-1, self.h_dim)) # (b, s, h) -> (b * s, 1)\n",
    "        return F.softmax(attn_ene.view(b_size, -1), dim=1).unsqueeze(2) # (b*s, 1) -> (b, s, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後にattentionを利用して実際に文書分類を行う部分です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnClassifier(nn.Module):\n",
    "    def __init__(self, h_dim, c_num):\n",
    "        super(AttnClassifier, self).__init__()\n",
    "        self.attn = Attn(h_dim)\n",
    "        self.main = nn.Linear(h_dim, c_num)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        attns = self.attn(encoder_outputs) #(b, s, 1)\n",
    "        feats = (encoder_outputs * attns).sum(dim=1) # (b, s, h) -> (b, h)\n",
    "        return F.log_softmax(self.main(feats),dim=1), attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習・検証を行う関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, train_iter, optimizer, log_interval=1, batch_size=2):\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    correct = 0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l), y = batch.Text, batch.Label\n",
    "        optimizer.zero_grad()\n",
    "        encoder_outputs = encoder(x)\n",
    "        output, attn = classifier(encoder_outputs)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "        if idx % log_interval == 0:\n",
    "            print('train epoch: {} [{}/{}], acc:{}, loss:{}'.format(\n",
    "            epoch, (idx+1)*len(x), len(train_iter)*batch_size,\n",
    "            correct/float(log_interval * len(x)),\n",
    "            loss.data[0]))\n",
    "            correct = 0\n",
    "\n",
    "            \n",
    "def test_model(epoch, test_iter):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    for idx, batch in enumerate(test_iter):\n",
    "        (x, x_l), y = batch.Text, batch.Label\n",
    "        encoder_outputs = encoder(x)\n",
    "        output, attn = classifier(encoder_outputs)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    print('test epoch:{}, acc:{}'.format(epoch, correct/float(len(test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習実行\n",
    "ハイパーパラメータなどを指定して学習を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN\n",
      "Embedding\n",
      "LSTM\n",
      "AttnClassifier\n",
      "Attn\n",
      "Sequential\n",
      "Linear\n",
      "ReLU\n",
      "Linear\n",
      "Linear\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 300 #単語埋め込み次元\n",
    "h_dim = 3 #lstmの隠れ層の次元\n",
    "class_num = 2 #予測クラウ数\n",
    "lr = 0.001 #学習係数\n",
    "epochs = 100 #エポック数\n",
    "\n",
    " # make model\n",
    "encoder = EncoderRNN(emb_dim, h_dim, len(TEXT.vocab),gpu=False, v_vec = TEXT.vocab.vectors)\n",
    "classifier = AttnClassifier(h_dim, class_num)\n",
    "\n",
    "# init model\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if hasattr(m, 'weight') and (classname.find('Embedding') == -1):\n",
    "        nn.init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "for m in encoder.modules():\n",
    "    print(m.__class__.__name__)\n",
    "    weights_init(m)\n",
    "    \n",
    "for m in classifier.modules():\n",
    "    print(m.__class__.__name__)\n",
    "    weights_init(m)\n",
    "\n",
    "# optim\n",
    "from itertools import chain\n",
    "optimizer = optim.Adam(chain(encoder.parameters(),classifier.parameters()), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 [2/4], acc:1.0, loss:0.560247540473938\n",
      "train epoch: 1 [4/4], acc:0.5, loss:1.0654399394989014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itok/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n",
      "/home/itok/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch:1, acc:0.75\n",
      "train epoch: 2 [2/4], acc:1.0, loss:0.5486541986465454\n",
      "train epoch: 2 [4/4], acc:0.5, loss:0.8221700191497803\n",
      "test epoch:2, acc:0.5\n",
      "train epoch: 3 [2/4], acc:0.5, loss:0.5928575992584229\n",
      "train epoch: 3 [4/4], acc:0.5, loss:0.6966612339019775\n",
      "test epoch:3, acc:0.5\n",
      "train epoch: 4 [2/4], acc:0.5, loss:0.6245954036712646\n",
      "train epoch: 4 [4/4], acc:0.5, loss:0.7563215494155884\n",
      "test epoch:4, acc:0.5\n",
      "train epoch: 5 [2/4], acc:0.5, loss:0.5691448450088501\n",
      "train epoch: 5 [4/4], acc:0.5, loss:0.730218768119812\n",
      "test epoch:5, acc:0.5\n",
      "train epoch: 6 [2/4], acc:0.5, loss:0.6600370407104492\n",
      "train epoch: 6 [4/4], acc:0.5, loss:0.6107431650161743\n",
      "test epoch:6, acc:0.5\n",
      "train epoch: 7 [2/4], acc:1.0, loss:0.47765815258026123\n",
      "train epoch: 7 [4/4], acc:0.0, loss:0.7952502965927124\n",
      "test epoch:7, acc:0.5\n",
      "train epoch: 8 [2/4], acc:0.5, loss:0.5760264992713928\n",
      "train epoch: 8 [4/4], acc:1.0, loss:0.6201963424682617\n",
      "test epoch:8, acc:0.75\n",
      "train epoch: 9 [2/4], acc:1.0, loss:0.5267462134361267\n",
      "train epoch: 9 [4/4], acc:0.5, loss:0.6337802410125732\n",
      "test epoch:9, acc:1.0\n",
      "train epoch: 10 [2/4], acc:1.0, loss:0.5881224870681763\n",
      "train epoch: 10 [4/4], acc:1.0, loss:0.4989398121833801\n",
      "test epoch:10, acc:1.0\n",
      "train epoch: 11 [2/4], acc:1.0, loss:0.4800560176372528\n",
      "train epoch: 11 [4/4], acc:1.0, loss:0.6003427505493164\n",
      "test epoch:11, acc:1.0\n",
      "train epoch: 12 [2/4], acc:1.0, loss:0.5668047070503235\n",
      "train epoch: 12 [4/4], acc:1.0, loss:0.46152418851852417\n",
      "test epoch:12, acc:1.0\n",
      "train epoch: 13 [2/4], acc:1.0, loss:0.5094179511070251\n",
      "train epoch: 13 [4/4], acc:0.5, loss:0.6625293493270874\n",
      "test epoch:13, acc:0.75\n",
      "train epoch: 14 [2/4], acc:0.5, loss:0.6012454032897949\n",
      "train epoch: 14 [4/4], acc:1.0, loss:0.5321455001831055\n",
      "test epoch:14, acc:1.0\n",
      "train epoch: 15 [2/4], acc:1.0, loss:0.4770073890686035\n",
      "train epoch: 15 [4/4], acc:1.0, loss:0.537368893623352\n",
      "test epoch:15, acc:1.0\n",
      "train epoch: 16 [2/4], acc:1.0, loss:0.4398651719093323\n",
      "train epoch: 16 [4/4], acc:1.0, loss:0.5719512701034546\n",
      "test epoch:16, acc:1.0\n",
      "train epoch: 17 [2/4], acc:1.0, loss:0.4713858962059021\n",
      "train epoch: 17 [4/4], acc:1.0, loss:0.5208694338798523\n",
      "test epoch:17, acc:1.0\n",
      "train epoch: 18 [2/4], acc:1.0, loss:0.46690040826797485\n",
      "train epoch: 18 [4/4], acc:1.0, loss:0.5135233998298645\n",
      "test epoch:18, acc:1.0\n",
      "train epoch: 19 [2/4], acc:1.0, loss:0.5091257095336914\n",
      "train epoch: 19 [4/4], acc:1.0, loss:0.4617699980735779\n",
      "test epoch:19, acc:1.0\n",
      "train epoch: 20 [2/4], acc:1.0, loss:0.41583144664764404\n",
      "train epoch: 20 [4/4], acc:1.0, loss:0.5412516593933105\n",
      "test epoch:20, acc:1.0\n",
      "train epoch: 21 [2/4], acc:1.0, loss:0.396310031414032\n",
      "train epoch: 21 [4/4], acc:1.0, loss:0.5244275331497192\n",
      "test epoch:21, acc:1.0\n",
      "train epoch: 22 [2/4], acc:1.0, loss:0.3870468735694885\n",
      "train epoch: 22 [4/4], acc:1.0, loss:0.5188115835189819\n",
      "test epoch:22, acc:1.0\n",
      "train epoch: 23 [2/4], acc:1.0, loss:0.509414792060852\n",
      "train epoch: 23 [4/4], acc:1.0, loss:0.40779170393943787\n",
      "test epoch:23, acc:1.0\n",
      "train epoch: 24 [2/4], acc:1.0, loss:0.4586063027381897\n",
      "train epoch: 24 [4/4], acc:1.0, loss:0.44095826148986816\n",
      "test epoch:24, acc:1.0\n",
      "train epoch: 25 [2/4], acc:1.0, loss:0.4388159513473511\n",
      "train epoch: 25 [4/4], acc:1.0, loss:0.44552743434906006\n",
      "test epoch:25, acc:1.0\n",
      "train epoch: 26 [2/4], acc:1.0, loss:0.44151145219802856\n",
      "train epoch: 26 [4/4], acc:1.0, loss:0.4317513108253479\n",
      "test epoch:26, acc:1.0\n",
      "train epoch: 27 [2/4], acc:1.0, loss:0.4334484338760376\n",
      "train epoch: 27 [4/4], acc:1.0, loss:0.4268881678581238\n",
      "test epoch:27, acc:1.0\n",
      "train epoch: 28 [2/4], acc:1.0, loss:0.4239967465400696\n",
      "train epoch: 28 [4/4], acc:1.0, loss:0.4231172204017639\n",
      "test epoch:28, acc:1.0\n",
      "train epoch: 29 [2/4], acc:1.0, loss:0.4198083281517029\n",
      "train epoch: 29 [4/4], acc:1.0, loss:0.41568171977996826\n",
      "test epoch:29, acc:1.0\n",
      "train epoch: 30 [2/4], acc:1.0, loss:0.43207603693008423\n",
      "train epoch: 30 [4/4], acc:1.0, loss:0.3949333727359772\n",
      "test epoch:30, acc:1.0\n",
      "train epoch: 31 [2/4], acc:1.0, loss:0.4080386161804199\n",
      "train epoch: 31 [4/4], acc:1.0, loss:0.4036039113998413\n",
      "test epoch:31, acc:1.0\n",
      "train epoch: 32 [2/4], acc:1.0, loss:0.3050202429294586\n",
      "train epoch: 32 [4/4], acc:1.0, loss:0.4673026204109192\n",
      "test epoch:32, acc:1.0\n",
      "train epoch: 33 [2/4], acc:1.0, loss:0.3966049253940582\n",
      "train epoch: 33 [4/4], acc:1.0, loss:0.39043131470680237\n",
      "test epoch:33, acc:1.0\n",
      "train epoch: 34 [2/4], acc:1.0, loss:0.2927308976650238\n",
      "train epoch: 34 [4/4], acc:1.0, loss:0.4571666717529297\n",
      "test epoch:34, acc:1.0\n",
      "train epoch: 35 [2/4], acc:1.0, loss:0.39505332708358765\n",
      "train epoch: 35 [4/4], acc:1.0, loss:0.3756178021430969\n",
      "test epoch:35, acc:1.0\n",
      "train epoch: 36 [2/4], acc:1.0, loss:0.3827705979347229\n",
      "train epoch: 36 [4/4], acc:1.0, loss:0.3745179772377014\n",
      "test epoch:36, acc:1.0\n",
      "train epoch: 37 [2/4], acc:1.0, loss:0.270023912191391\n",
      "train epoch: 37 [4/4], acc:1.0, loss:0.44169706106185913\n",
      "test epoch:37, acc:1.0\n",
      "train epoch: 38 [2/4], acc:1.0, loss:0.3683009743690491\n",
      "train epoch: 38 [4/4], acc:1.0, loss:0.3664872944355011\n",
      "test epoch:38, acc:1.0\n",
      "train epoch: 39 [2/4], acc:1.0, loss:0.3671063184738159\n",
      "train epoch: 39 [4/4], acc:1.0, loss:0.35392239689826965\n",
      "test epoch:39, acc:1.0\n",
      "train epoch: 40 [2/4], acc:1.0, loss:0.25152677297592163\n",
      "train epoch: 40 [4/4], acc:1.0, loss:0.4267536401748657\n",
      "test epoch:40, acc:1.0\n",
      "train epoch: 41 [2/4], acc:1.0, loss:0.35193753242492676\n",
      "train epoch: 41 [4/4], acc:1.0, loss:0.349376916885376\n",
      "test epoch:41, acc:1.0\n",
      "train epoch: 42 [2/4], acc:1.0, loss:0.3450738191604614\n",
      "train epoch: 42 [4/4], acc:1.0, loss:0.34485316276550293\n",
      "test epoch:42, acc:1.0\n",
      "train epoch: 43 [2/4], acc:1.0, loss:0.3404327630996704\n",
      "train epoch: 43 [4/4], acc:1.0, loss:0.34082794189453125\n",
      "test epoch:43, acc:1.0\n",
      "train epoch: 44 [2/4], acc:1.0, loss:0.22842274606227875\n",
      "train epoch: 44 [4/4], acc:1.0, loss:0.4065777659416199\n",
      "test epoch:44, acc:1.0\n",
      "train epoch: 45 [2/4], acc:1.0, loss:0.22298233211040497\n",
      "train epoch: 45 [4/4], acc:1.0, loss:0.40139031410217285\n",
      "test epoch:45, acc:1.0\n",
      "train epoch: 46 [2/4], acc:1.0, loss:0.3164560794830322\n",
      "train epoch: 46 [4/4], acc:1.0, loss:0.329827219247818\n",
      "test epoch:46, acc:1.0\n",
      "train epoch: 47 [2/4], acc:1.0, loss:0.32723933458328247\n",
      "train epoch: 47 [4/4], acc:1.0, loss:0.3078632950782776\n",
      "test epoch:47, acc:1.0\n",
      "train epoch: 48 [2/4], acc:1.0, loss:0.20685209333896637\n",
      "train epoch: 48 [4/4], acc:1.0, loss:0.38541293144226074\n",
      "test epoch:48, acc:1.0\n",
      "train epoch: 49 [2/4], acc:1.0, loss:0.3826914131641388\n",
      "train epoch: 49 [4/4], acc:1.0, loss:0.19867682456970215\n",
      "test epoch:49, acc:1.0\n",
      "train epoch: 50 [2/4], acc:1.0, loss:0.19591796398162842\n",
      "train epoch: 50 [4/4], acc:1.0, loss:0.37436607480049133\n",
      "test epoch:50, acc:1.0\n",
      "train epoch: 51 [2/4], acc:1.0, loss:0.3715316951274872\n",
      "train epoch: 51 [4/4], acc:1.0, loss:0.1880120187997818\n",
      "test epoch:51, acc:1.0\n",
      "train epoch: 52 [2/4], acc:1.0, loss:0.18515071272850037\n",
      "train epoch: 52 [4/4], acc:1.0, loss:0.3633147180080414\n",
      "test epoch:52, acc:1.0\n",
      "train epoch: 53 [2/4], acc:1.0, loss:0.2852674722671509\n",
      "train epoch: 53 [4/4], acc:1.0, loss:0.2860070765018463\n",
      "test epoch:53, acc:1.0\n",
      "train epoch: 54 [2/4], acc:1.0, loss:0.2786867320537567\n",
      "train epoch: 54 [4/4], acc:1.0, loss:0.2824089229106903\n",
      "test epoch:54, acc:1.0\n",
      "train epoch: 55 [2/4], acc:1.0, loss:0.1709362119436264\n",
      "train epoch: 55 [4/4], acc:1.0, loss:0.34731125831604004\n",
      "test epoch:55, acc:1.0\n",
      "train epoch: 56 [2/4], acc:1.0, loss:0.16655060648918152\n",
      "train epoch: 56 [4/4], acc:1.0, loss:0.34205684065818787\n",
      "test epoch:56, acc:1.0\n",
      "train epoch: 57 [2/4], acc:1.0, loss:0.3393133580684662\n",
      "train epoch: 57 [4/4], acc:1.0, loss:0.15882046520709991\n",
      "test epoch:57, acc:1.0\n",
      "train epoch: 58 [2/4], acc:1.0, loss:0.2513241469860077\n",
      "train epoch: 58 [4/4], acc:1.0, loss:0.2707501947879791\n",
      "test epoch:58, acc:1.0\n",
      "train epoch: 59 [2/4], acc:1.0, loss:0.2605874240398407\n",
      "train epoch: 59 [4/4], acc:1.0, loss:0.2534194588661194\n",
      "test epoch:59, acc:1.0\n",
      "train epoch: 60 [2/4], acc:1.0, loss:0.2509955167770386\n",
      "train epoch: 60 [4/4], acc:1.0, loss:0.2526598572731018\n",
      "test epoch:60, acc:1.0\n",
      "train epoch: 61 [2/4], acc:1.0, loss:0.2513234615325928\n",
      "train epoch: 61 [4/4], acc:1.0, loss:0.23948943614959717\n",
      "test epoch:61, acc:1.0\n",
      "train epoch: 62 [2/4], acc:1.0, loss:0.25057122111320496\n",
      "train epoch: 62 [4/4], acc:1.0, loss:0.2297566831111908\n",
      "test epoch:62, acc:1.0\n",
      "train epoch: 63 [2/4], acc:1.0, loss:0.14087042212486267\n",
      "train epoch: 63 [4/4], acc:1.0, loss:0.3054037094116211\n",
      "test epoch:63, acc:1.0\n",
      "train epoch: 64 [2/4], acc:1.0, loss:0.24494896829128265\n",
      "train epoch: 64 [4/4], acc:1.0, loss:0.21594715118408203\n",
      "test epoch:64, acc:1.0\n",
      "train epoch: 65 [2/4], acc:1.0, loss:0.21164436638355255\n",
      "train epoch: 65 [4/4], acc:1.0, loss:0.24299515783786774\n",
      "test epoch:65, acc:1.0\n",
      "train epoch: 66 [2/4], acc:1.0, loss:0.22838586568832397\n",
      "train epoch: 66 [4/4], acc:1.0, loss:0.21268312633037567\n",
      "test epoch:66, acc:1.0\n",
      "train epoch: 67 [2/4], acc:1.0, loss:0.21100404858589172\n",
      "train epoch: 67 [4/4], acc:1.0, loss:0.22149205207824707\n",
      "test epoch:67, acc:1.0\n",
      "train epoch: 68 [2/4], acc:1.0, loss:0.18448635935783386\n",
      "train epoch: 68 [4/4], acc:1.0, loss:0.2416454404592514\n",
      "test epoch:68, acc:1.0\n",
      "train epoch: 69 [2/4], acc:1.0, loss:0.13092458248138428\n",
      "train epoch: 69 [4/4], acc:1.0, loss:0.2767750024795532\n",
      "test epoch:69, acc:1.0\n",
      "train epoch: 70 [2/4], acc:1.0, loss:0.20946748554706573\n",
      "train epoch: 70 [4/4], acc:1.0, loss:0.19629481434822083\n",
      "test epoch:70, acc:1.0\n",
      "train epoch: 71 [2/4], acc:1.0, loss:0.2698199450969696\n",
      "train epoch: 71 [4/4], acc:1.0, loss:0.1243094950914383\n",
      "test epoch:71, acc:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 72 [2/4], acc:1.0, loss:0.1633222997188568\n",
      "train epoch: 72 [4/4], acc:1.0, loss:0.2283763885498047\n",
      "test epoch:72, acc:1.0\n",
      "train epoch: 73 [2/4], acc:1.0, loss:0.12003561854362488\n",
      "train epoch: 73 [4/4], acc:1.0, loss:0.2560301423072815\n",
      "test epoch:73, acc:1.0\n",
      "train epoch: 74 [2/4], acc:1.0, loss:0.15971876680850983\n",
      "train epoch: 74 [4/4], acc:1.0, loss:0.21789105236530304\n",
      "test epoch:74, acc:1.0\n",
      "train epoch: 75 [2/4], acc:1.0, loss:0.19169685244560242\n",
      "train epoch: 75 [4/4], acc:1.0, loss:0.17677609622478485\n",
      "test epoch:75, acc:1.0\n",
      "train epoch: 76 [2/4], acc:1.0, loss:0.2084178626537323\n",
      "train epoch: 76 [4/4], acc:1.0, loss:0.1558011770248413\n",
      "test epoch:76, acc:1.0\n",
      "train epoch: 77 [2/4], acc:1.0, loss:0.24056687951087952\n",
      "train epoch: 77 [4/4], acc:1.0, loss:0.10514883697032928\n",
      "test epoch:77, acc:1.0\n",
      "train epoch: 78 [2/4], acc:1.0, loss:0.19618894159793854\n",
      "train epoch: 78 [4/4], acc:1.0, loss:0.15393829345703125\n",
      "test epoch:78, acc:1.0\n",
      "train epoch: 79 [2/4], acc:1.0, loss:0.23059400916099548\n",
      "train epoch: 79 [4/4], acc:1.0, loss:0.09933311492204666\n",
      "test epoch:79, acc:1.0\n",
      "train epoch: 80 [2/4], acc:1.0, loss:0.22689953446388245\n",
      "train epoch: 80 [4/4], acc:1.0, loss:0.0963745191693306\n",
      "test epoch:80, acc:1.0\n",
      "train epoch: 81 [2/4], acc:1.0, loss:0.22260326147079468\n",
      "train epoch: 81 [4/4], acc:1.0, loss:0.09367197751998901\n",
      "test epoch:81, acc:1.0\n",
      "train epoch: 82 [2/4], acc:1.0, loss:0.15489572286605835\n",
      "train epoch: 82 [4/4], acc:1.0, loss:0.16897530853748322\n",
      "test epoch:82, acc:1.0\n",
      "train epoch: 83 [2/4], acc:1.0, loss:0.08883041143417358\n",
      "train epoch: 83 [4/4], acc:1.0, loss:0.21198885142803192\n",
      "test epoch:83, acc:1.0\n",
      "train epoch: 84 [2/4], acc:1.0, loss:0.08598301559686661\n",
      "train epoch: 84 [4/4], acc:1.0, loss:0.20841434597969055\n",
      "test epoch:84, acc:1.0\n",
      "train epoch: 85 [2/4], acc:1.0, loss:0.14565691351890564\n",
      "train epoch: 85 [4/4], acc:1.0, loss:0.16468243300914764\n",
      "test epoch:85, acc:1.0\n",
      "train epoch: 86 [2/4], acc:1.0, loss:0.1625775247812271\n",
      "train epoch: 86 [4/4], acc:1.0, loss:0.14154018461704254\n",
      "test epoch:86, acc:1.0\n",
      "train epoch: 87 [2/4], acc:1.0, loss:0.19865882396697998\n",
      "train epoch: 87 [4/4], acc:1.0, loss:0.07935745269060135\n",
      "test epoch:87, acc:1.0\n",
      "train epoch: 88 [2/4], acc:1.0, loss:0.1406288743019104\n",
      "train epoch: 88 [4/4], acc:1.0, loss:0.15249578654766083\n",
      "test epoch:88, acc:1.0\n",
      "train epoch: 89 [2/4], acc:1.0, loss:0.13408419489860535\n",
      "train epoch: 89 [4/4], acc:1.0, loss:0.15300396084785461\n",
      "test epoch:89, acc:1.0\n",
      "train epoch: 90 [2/4], acc:1.0, loss:0.1474977731704712\n",
      "train epoch: 90 [4/4], acc:1.0, loss:0.13063037395477295\n",
      "test epoch:90, acc:1.0\n",
      "train epoch: 91 [2/4], acc:1.0, loss:0.1831619143486023\n",
      "train epoch: 91 [4/4], acc:1.0, loss:0.07900610566139221\n",
      "test epoch:91, acc:1.0\n",
      "train epoch: 92 [2/4], acc:1.0, loss:0.11474058032035828\n",
      "train epoch: 92 [4/4], acc:1.0, loss:0.15592196583747864\n",
      "test epoch:92, acc:1.0\n",
      "train epoch: 93 [2/4], acc:1.0, loss:0.10912035405635834\n",
      "train epoch: 93 [4/4], acc:1.0, loss:0.14985303580760956\n",
      "test epoch:93, acc:1.0\n",
      "train epoch: 94 [2/4], acc:1.0, loss:0.13245205581188202\n",
      "train epoch: 94 [4/4], acc:1.0, loss:0.12088477611541748\n",
      "test epoch:94, acc:1.0\n",
      "train epoch: 95 [2/4], acc:1.0, loss:0.12934929132461548\n",
      "train epoch: 95 [4/4], acc:1.0, loss:0.11857219785451889\n",
      "test epoch:95, acc:1.0\n",
      "train epoch: 96 [2/4], acc:1.0, loss:0.16609463095664978\n",
      "train epoch: 96 [4/4], acc:1.0, loss:0.07609120011329651\n",
      "test epoch:96, acc:1.0\n",
      "train epoch: 97 [2/4], acc:1.0, loss:0.09259477257728577\n",
      "train epoch: 97 [4/4], acc:1.0, loss:0.14496134221553802\n",
      "test epoch:97, acc:1.0\n",
      "train epoch: 98 [2/4], acc:1.0, loss:0.08908535540103912\n",
      "train epoch: 98 [4/4], acc:1.0, loss:0.14451363682746887\n",
      "test epoch:98, acc:1.0\n",
      "train epoch: 99 [2/4], acc:1.0, loss:0.1429513543844223\n",
      "train epoch: 99 [4/4], acc:1.0, loss:0.08564858138561249\n",
      "test epoch:99, acc:1.0\n",
      "train epoch: 100 [2/4], acc:1.0, loss:0.15593023598194122\n",
      "train epoch: 100 [4/4], acc:1.0, loss:0.07290943711996078\n",
      "test epoch:100, acc:1.0\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(epochs):\n",
    "    train_model(epoch + 1, train_iter, optimizer)\n",
    "    test_model(epoch + 1, val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attentionの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spanタグを利用して、attentionを可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# 単語とattentionの強さを受け取ってspanタグをつける関数\n",
    "def highlight(word, attn):\n",
    "    html_color = u'#%02X%02X%02X' % (255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "    return u'<span style=\"background-color: {}\">{}</span>'.format(html_color, word)\n",
    "\n",
    "# 単語番号と対応するattentionの配列を受け取って、spanタグで色付けされた文字列を返す関数\n",
    "def mk_html(sentence, attns):\n",
    "    def itos(word):\n",
    "        word = TEXT.vocab.itos[word]\n",
    "        return word.strip(\"<\").strip(\">\")\n",
    "    html = u\"\"\n",
    "    for word, attn in zip(sentence, attns):\n",
    "        html += u' ' + highlight(\n",
    "        itos(word),\n",
    "        attn\n",
    "        )\n",
    "    return html + u\"<br><br>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itok/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "正解1:予測1 <span style=\"background-color: #FFABAB\">unk</span> <span style=\"background-color: #FFACAC\">unk</span> <span style=\"background-color: #FFE8E8\">が</span> <span style=\"background-color: #FFE8E8\">好き</span> <span style=\"background-color: #FFE8E8\">です</span> <span style=\"background-color: #FFE8E8\">。</span><br><br>正解0:予測0 <span style=\"background-color: #FFF3F3\">私</span> <span style=\"background-color: #FFF3F3\">は</span> <span style=\"background-color: #FFDDDD\">unk</span> <span style=\"background-color: #FFF3F3\">が</span> <span style=\"background-color: #FF5858\">嫌い</span> <span style=\"background-color: #FFF3F3\">です</span> <span style=\"background-color: #FFF3F3\">。</span><br><br>正解1:予測1 <span style=\"background-color: #FFE2E2\">私</span> <span style=\"background-color: #FFE2E2\">は</span> <span style=\"background-color: #FFAAAA\">unk</span> <span style=\"background-color: #FFE2E2\">が</span> <span style=\"background-color: #FFE2E2\">好き</span> <span style=\"background-color: #FFE2E2\">です</span> <span style=\"background-color: #FFE2E2\">。</span><br><br>正解0:予測0 <span style=\"background-color: #FFD8D8\">unk</span> <span style=\"background-color: #FFF2F2\">が</span> <span style=\"background-color: #FF4A4A\">嫌い</span> <span style=\"background-color: #FFF2F2\">です</span> <span style=\"background-color: #FFF2F2\">。</span><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = \"\"\n",
    "for batch in test_iter:\n",
    "    x = batch.Text[0]\n",
    "    y = batch.Label\n",
    "    encoder_outputs = encoder(x)\n",
    "    output, attn = classifier(encoder_outputs)\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    a = attn.data[0,:,0]\n",
    "    res += u\"正解{}:予測{}\".format(str(y[0].data[0]), str(pred[0][0])) + mk_html(x.data[0], a)\n",
    "HTML(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
